# box_dataset.py

"""Generates a synthetic dataset of moving, expanding, and intensifying squares.

This module provides a PyTorch Dataset class, `BoxDataSet`, which creates a
synthetic dataset as described in the source documentation. Each dataset
element is a sequence of images depicting a square that moves, grows, and
increases in pixel intensity over time on a zero-valued background.

The data generation logic in this file is a direct, cleaned-up implementation
of the original source code to ensure functionally identical output.
"""

from typing import Tuple
import torch
from torch.utils.data import Dataset
import numpy as np
import random

# --- Constants for Dataset Generation (Derived from original source code) ---

# Image and Split Configuration
_IMAGE_SIZE = 64
_TRAIN_TEST_SPLIT_RATIO = 0.8

# Square Initial Parameter Constraints
_MIN_INIT_SQUARE_SIZE = 3
_MAX_INIT_SQUARE_SIZE = 8
_MIN_INIT_OFFSET = 1
_MAX_INIT_OFFSET = 20

# Square Animation Parameter Constraints
_MIN_GROWTH_RATE = 2
_MAX_GROWTH_RATE = 5

# Pixel Intensity Scaling (Derived from `torch.tensor(np.arange(1, 9)) * 0.08`)
_NUM_SCALED_FRAMES = 8
_PIXEL_SCALE_FACTOR = 0.08


class BoxDataSet(Dataset):
    """A PyTorch Dataset for generating sequences of moving squares.

    This class creates a synthetic dataset where each sample is a sequence of
    single-channel images. A square within the images moves towards one of
    the four corners, expands in size, and its pixel values increase over the
    sequence. The logic for data generation is functionally identical to the
    original source to ensure exact replication of the dataset.

    Attributes:
        x: The tensor containing the input sequences.
        y: The tensor containing the target output sequences.
    """

    def __init__(
        self,
        num_examples: int,
        in_seq_len: int,
        out_seq_len: int,
        mode: str = "test",
    ) -> None:
        """Initializes the dataset and generates all data within this method.

        Args:
            num_examples: The total number of sequences to generate.
            in_seq_len: The number of frames for the input sequence.
            out_seq_len: The number of frames for the target output sequence.
            mode: The dataset mode, either "train" or "test".

        Raises:
            ValueError: If the mode is not "train" or "test".
        """
        super().__init__()
        if mode not in ("train", "test"):
            raise ValueError('Mode must be either "train" or "test".')

        self.total_seq_len = in_seq_len + out_seq_len
        
        # --- Data Generation Logic (Mirrors original structure) ---

        # 1. Pre-allocate a numpy array for the full sequences
        all_sequences = np.zeros(
            (num_examples, self.total_seq_len, 1, _IMAGE_SIZE, _IMAGE_SIZE),
            dtype=np.float32
        )

        for i in range(all_sequences.shape[0]):
            # 2. Initialize random parameters for the sequence
            corner = random.randint(0, 3)
            start_size = random.randint(_MIN_INIT_SQUARE_SIZE, _MAX_INIT_SQUARE_SIZE)
            x_off = random.randint(_MIN_INIT_OFFSET, _MAX_INIT_OFFSET)
            y_off = random.randint(_MIN_INIT_OFFSET, _MAX_INIT_OFFSET)
            shift = random.randint(_MIN_GROWTH_RATE, _MAX_GROWTH_RATE)
            
            for t in range(all_sequences.shape[1]):
                # 3. Calculate current size and draw the square
                size = start_size + t * shift
                if corner == 0:  # Top-left anchor
                    all_sequences[i, t, 0, y_off:y_off + size, x_off:x_off + size] = 1.0
                elif corner == 1:  # Top-right anchor
                    all_sequences[i, t, 0, y_off:y_off + size, x_off - size:x_off] = 1.0
                elif corner == 2:  # Bottom-left anchor
                    all_sequences[i, t, 0, y_off - size:y_off, x_off:x_off + size] = 1.0
                elif corner == 3:  # Bottom-right anchor
                    all_sequences[i, t, 0, y_off - size:y_off, x_off - size:x_off] = 1.0

                # 4. Update offsets for the next frame, stopping at boundaries.
                # This logic is now an explicit, direct copy of the original's
                # behavior to ensure correctness.
                if corner == 0:  # Moving towards top-left
                    if y_off > 0: y_off -= 1
                    if x_off > 0: x_off -= 1
                elif corner == 1:  # Moving towards top-right
                    if y_off > 0: y_off -= 1
                    if x_off < _IMAGE_SIZE: x_off += 1
                elif corner == 2:  # Moving towards bottom-left
                    if y_off < _IMAGE_SIZE: y_off += 1
                    if x_off > 0: x_off -= 1
                elif corner == 3:  # Moving towards bottom-right
                    if y_off < _IMAGE_SIZE: y_off += 1
                    if x_off < _IMAGE_SIZE: x_off += 1


        # 5. Scale pixel values over the time dimension
        scale_values = torch.arange(1, _NUM_SCALED_FRAMES + 1, dtype=torch.float32) * _PIXEL_SCALE_FACTOR
        multiplier = scale_values.view(1, _NUM_SCALED_FRAMES, 1, 1, 1)
        
        all_sequences_torch = torch.from_numpy(all_sequences)
        # Note: Original logic only scales the first N frames of the sequence.
        num_frames_to_scale = min(self.total_seq_len, _NUM_SCALED_FRAMES)
        all_sequences_torch[:, :num_frames_to_scale] *= multiplier[:,:num_frames_to_scale]

        # 6. Split into train/test and input/target sets
        num_train = int(_TRAIN_TEST_SPLIT_RATIO * num_examples)
        if mode == "train":
            self.x = all_sequences_torch[:num_train, :in_seq_len, ...]
            self.y = all_sequences_torch[:num_train, in_seq_len:, ...]
        else:
            self.x = all_sequences_torch[num_train:, :in_seq_len, ...]
            self.y = all_sequences_torch[num_train:, in_seq_len:, ...]

    def __len__(self) -> int:
        """Returns the number of samples in the dataset."""
        return len(self.x)

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Retrieves a single sample from the dataset.

        Args:
            index: The index of the sample to retrieve.

        Returns:
            A tuple containing the input sequence tensor and the
            corresponding output sequence tensor.
        """
        in_seq_tensor = self.x[index]
        out_seq_tensor = self.y[index]
        return in_seq_tensor, out_seq_tensor
