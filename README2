AI/ML question: Is there a known issue training models if I have say one feature that a big number O(100k) and one feature that is tiny O(1e-6)?
 
 
you typically want to avoid that
 
and normalize your data
 
This makes sense why my GRDTH frame is working when I use the scalar.
 
Yeah, generally you want to normalize. Hypothetically I'd think a NN would detect and weight the variables to normalize it within the first step, but almost every NN lesson/study I've seen says to normalize first
 
Zach is spot on. Normalization layers can help, but general rule of thumb is to normalize inputs since the larger magnitude values will be more influential through the forward pass (especially in full connected layers because everything is matrix multiplied together more or less and weights are usually initialized to small values between -1 and 1) than the lower magnitude values and therefor impact weight updates. So you might see that no matter what the values are for those O(1-6) they don't influence the model training
 
