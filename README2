
"""Time Series Predictor."""

import os
from collections.abc import Mapping
from copy import copy
from pathlib import Path
from typing import Any, Optional, Union

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import (
    EarlyStopping,
    ModelCheckpoint,
    ReduceLROnPlateau,
)
from tensorflow.keras.layers import (
    LSTM,
    Activation,
    Dense,
    Input,
    RepeatVector,
    TimeDistributed,
)
from tensorflow.keras.models import load_model

from prediction.base_predictor import BasePredictor
from prediction.configurations.time_series_predictor_configuration import (
    TimeSeriesPredictorConfiguration,
)
from prediction.configurations.ved_predictor_configuration import (
    VEDPredictorConfiguration,
)
from prediction.data_types.checkpoint_type import CheckpointType
from prediction.data_types.metric_type import MetricType
from prediction.mlearn.ved_predictor import SamplingLayer, VEDPredictor
from prediction.util.time_series_predictor_preprocessing import (
    closest_to_impact,
    has_reached_impact,
    impact_location_to_zero,
    to_sequences,
)


class TimeSeriesPredictor(BasePredictor):
    """Class for handling time-series prediction.

    Capable of handling input and output sequences of varying length. The
    general way to handle time series prediction is to train an LSTM to predict
    the next time sequence and then use that prediction to make the following
    prediction. The class allows you to predict the next N time-steps in the
    future.
    """

    def __init__(
        self,
        configuration: TimeSeriesPredictorConfiguration,
    ) -> None:
        """Constructs a BasePredictor instance with a BaseConfiguration.

        Args:
            configuration: Configuration containing algorithm hyper-parameters.
        """
        super().__init__(configuration)

        self.num_features = self._configuration.num_features
        self.latent_dim = self._configuration.latent_dim
        self.hidden_units_1 = self._configuration.hidden_units_1
        self.hidden_units_2 = self._configuration.hidden_units_2
        self.input_sequence_length = self._configuration.input_sequence_length
        self.output_sequence_length = self._configuration.output_sequence_length

        if self._configuration.deterministic:
            self.model = self.deterministic_model()
        else:
            self.model = self.stochastic_model()

    def preprocess_train_trajectories(
        self,
        trajectories: Union[np.ndarray, list[np.ndarray]],
        in_seq_len: int,
        out_seq_len: int,
        shift: int = 5,
        train_split: float = 0.8,
    ) -> tuple:
        """Preprocess list of trajectories for training.

        Each trajectory is split into several sub-trajectories

        Args:
            self: Class self reference
            trajectories: List of numpy arrays (Can be various lengths)
            in_seq_len: Number of time steps for input data (int)
            out_seq_len: Number of time steps for output data (int)
            shift: Window shift size for splitting trajectory into
                    smaller sub-trajectories
            train_split: Percentage of data used for training

        Returns:
            x_train: Train input data (train examples, times-steps, features)
            y_train: Train output data (train examples, times-steps, features)
            x_test: Test input data (train examples, times-steps, features)
            y_test: Test output data (train examples, times-steps, features)
        """
        trajectories_copy = copy(trajectories)

        shifted_trajectories, _ = impact_location_to_zero(trajectories_copy)
        scaled_shifted_trajectories = copy(shifted_trajectories)

        for i in range(len(shifted_trajectories)):
            max_value = np.max(shifted_trajectories[i])
            scaler = np.floor(np.log10(max_value))
            divisor = np.power(10, scaler)
            scaled_shifted_trajectories[i] = shifted_trajectories[i] / divisor

        train, test = train_test_split(
            scaled_shifted_trajectories, train_size=train_split
        )

        x_train, y_train = to_sequences(
            train, in_seq_len, out_seq_len, shift=shift
        )
        x_valid, y_valid = to_sequences(
            test, in_seq_len, out_seq_len, shift=shift
        )

        return x_train, y_train, x_valid, y_valid

    def preprocess_test_trajectory(
        self,
        trajectory: np.ndarray,
        impact_location: np.ndarray,
    ) -> tuple:
        """Preprocesses a single trajectory.

        Trajectory to be used with infer_to_impact().

        Args:
            self: Class self reference
            trajectory: Single trajectory with shape (1,time-steps,features)
            impact_location: Impact location coordinate (numpy array)

        Returns:
            scaled_shifted_trajectory: Preprocessed trajectory with same shape
            scaler: Preprocessed trajectory was scaled down
                        by 10^scaler
        """
        full_traj = copy(trajectory)
        if full_traj.ndim == 2:
            full_traj = np.expand_dims(full_traj, axis=0)

        if impact_location.ndim == 2:
            impact_location = np.expand_dims(impact_location, axis=0)

        full_traj = np.concatenate((full_traj, impact_location), axis=1)

        shifted_trajectory, shift = impact_location_to_zero(full_traj)

        max_value = np.max(shifted_trajectory)
        scaler = np.floor(np.log10(max_value))
        scaled_shifted_trajectory = shifted_trajectory / np.power(10, scaler)

        return scaled_shifted_trajectory[:, :-1, :], scaler

    def fit(
        self,
        x: np.ndarray,
        y: np.ndarray,
        additional_data: Optional[tuple[np.ndarray, np.ndarray]] = None,
    ) -> tf.keras.callbacks.History:
        """Fit model to training data.

        Args:
            self: Class self reference
            x: Input data. Numpy array with shape (examples, times-steps,
                features).
            y: Output data. Numpy array with shape (examples, times-steps,
                features).
            additional_data: Tuple of test data with same format to train data.

        Returns:
            Dict with training results for each epoch.
        """
        if not (x.ndim == 3):
            raise ValueError(
                "x must have 3 dimensions (examples, timesteps, features)"
            )

        if not (y.ndim == 3):
            raise ValueError(
                "y must have 3 dimensions (examples, timesteps, features)"
            )

        if not ((x.shape[0] == y.shape[0]) and (x.shape[2] == y.shape[2])):
            raise ValueError("x and y must match dimensions along axis 0 and 2")

        if not (x.shape[2] == self.num_features):
            raise ValueError(
                "x.shape[2] should be same size as the models num_features"
            )

        if not (y.shape[1] == self.output_sequence_length):
            raise ValueError(
                "y.shape[1] should be same size as the models\
                    output_sequence_length"
            )

        if additional_data is not None:
            self._check_validation_data(additional_data)

        # Set model callbacks for early stopping and learning rate reduction.
        if self._configuration.deterministic:
            if callable(self._configuration.loss):
                monitor_val = self._configuration.loss.__name__
            else:
                monitor_val = self._configuration.loss.value
        else:
            monitor_val = "mean_absolute_error"
        monitor_val = "val_" + monitor_val

        callbacks_ = [
            EarlyStopping(
                monitor=monitor_val,
                mode="min",
                patience=self._configuration.early_stopping_patience,
                verbose=1,
                restore_best_weights=True,
            ),
            ReduceLROnPlateau(
                monitor=monitor_val,
                mode="min",
                patience=self._configuration.reduce_lr_on_plateau_patience,
                verbose=1,
            ),
        ]

        # Add checkpoint callback if configured.
        if (
            hasattr(self._configuration, "checkpoint")
            and self._configuration.checkpoint is not None
        ):
            checkpoint_filepath = self._configuration.checkpoint

            # Default to saving best model weights.
            save_best = True
            if self._configuration.checkpoint_type == CheckpointType.LAST:
                save_best = False

            model_checkpoint_callback = ModelCheckpoint(
                filepath=checkpoint_filepath,
                save_weights_only=True,
                monitor=monitor_val,
                mode="min",
                save_best_only=save_best,
            )

            callbacks_.append(model_checkpoint_callback)

        # Set validation split if no validation data was provided. If validation
        # was provided, the validation split will be None.
        val_split = 0.1 if additional_data is None else None
        return self.model.fit(
            x,
            y,
            validation_data=additional_data,
            validation_split=val_split,
            callbacks=callbacks_,
            epochs=self._configuration.num_epochs,
            batch_size=self._configuration.batch_size,
        )

    def infer(
        self,
        x: np.ndarray,
        y: Optional[np.ndarray] = None,
        additional_data: Optional[np.ndarray] = None,
        num_predictions: int = 1,
        num_trajectories: int = 1,
        scaler: int = 0,
        epsilon: float = 0.01,
    ) -> Union[np.ndarray, list[np.ndarray]]:
        """Make an inference for the given input data.

        Capable of making multiple future predictions of the
        output_sequence_length using a sliding window. Prediction results are
        used as input for the next inference. Inference can be directed towards
        a certain value with the impact_location parameter. When using an
        impact location, only a single trajectory may be passed.

        Args:
            self: Class self reference
            x: Input data. Numpy array with shape (examples, times steps,
                features).
            y: Impact location coordinate.
            additional_data: Additional data needed for inference.
            num_predictions: The number of predictions to make for the input
                trajectory data.
            num_trajectories: The number of trajectories to return. If using a
                deterministic model, all predicted trajectories are the same. If
                using a stochastic model, all predicted trajectories will be
                unique.
            scaler: Scaling factor for predictions, predicted values
                are multiplied by 10^scaler.
            epsilon: Float value that varies how close you must get to the
                impact location for early stopping. Smaller value means you must
                get closer to impact to satisfy early stopping.

        Returns:
            The inference results as a numpy array with shape (examples, times
            steps, features).
        """
        impact_location: Optional[np.ndarray] = y

        if not (x.shape[2] == self.num_features):
            raise ValueError(
                "x.shape[2] should be same size as the models num_features"
            )

        if (impact_location is not None) and (x.shape[0] != 1):
            raise ValueError(
                "When using impact_location, only 1 trajectory can be input."
            )

        trajectories_list = []
        to_impact = impact_location is not None
        for _ in range(num_trajectories):
            y_pred = np.copy(x)
            input_size = y_pred.shape[1]
            zeros = np.array([0, 0, 0])
            impacted = False
            offset = 0
            output_length = 0
            if to_impact and input_size > self.input_sequence_length:
                offset = input_size - self.input_sequence_length

            j = 0
            while j < num_predictions:
                start = j * self.output_sequence_length + offset
                stop = start + self.input_sequence_length + offset

                pred = self.model.predict(y_pred[:, start:stop, :])
                y_pred = np.concatenate([y_pred, pred], axis=1)
                j = j + 1

                if to_impact:
                    impact_index = has_reached_impact(pred, zeros, epsilon)
                    if impact_index > 0:
                        output_length = output_length + impact_index
                        impacted = True
                        break
                output_length = output_length + self.output_sequence_length

            start = input_size
            stop = start + output_length
            if to_impact and not impacted:
                stop = closest_to_impact(y_pred, zeros)
            y_pred = y_pred * np.power(10, scaler)

            if to_impact:
                y_pred = y_pred + impact_location
            trajectories_list.append(y_pred[:, start:stop, :])

        if num_trajectories == 1:
            return trajectories_list[0]

        return trajectories_list

    def save(self, checkpoint_path: str, *, weights_only: bool = True) -> None:
        """Save model weights by default to a checkpoint.

        If weights_only is False, the whole model
        will be saved

        Args:
            self: Class self reference
            checkpoint_path: String with name of checkpoint
            weights_only: Save weights only if true, else saves full model

        Returns:
            None
        """
        if weights_only:
            self.model.save_weights(checkpoint_path)
        else:
            self.model.save(checkpoint_path)

    def load(self, checkpoint_path: str, *, weights_only: bool = True) -> None:
        """Load model weights by default from a checkpoint.

        If weights_only is False, the whole model
        will be loaded

        Args:
            self: Class self reference
            checkpoint_path: String with name of checkpoint
            weights_only: Load weights only or full model

        Returns:
            None
        """
        if weights_only:
            self.model.load_weights(checkpoint_path)
        else:
            self.model = load_model(checkpoint_path)

    def deterministic_model(self) -> tf.keras.Model:
        """Creates deterministic autoencoder model.

        Returns:
            A deterministic time distributed Keras model.
        """
        # Encoder model.
        encoder_input = Input(shape=(None, self.num_features), name="input")
        x = LSTM(
            self.hidden_units_1,
            return_sequences=False,
            stateful=False,
        )(encoder_input)

        # Decoder model.
        x = RepeatVector(self.output_sequence_length)(x)
        x = LSTM(
            self.hidden_units_2,
            return_sequences=True,
            stateful=False,
        )(x)
        x = TimeDistributed(Dense(self.latent_dim))(x)
        x = TimeDistributed(Dense(self.num_features))(x)
        decoder_output = Activation("linear", name="output")(x)

        # Autoencoder model.
        autoencoder = tf.keras.Model(
            encoder_input, decoder_output, name="decoder"
        )

        # Compile the autoencoder model.
        return self._compile_model(autoencoder)

    def stochastic_model(self) -> Any:
        """Creates VED model.

        Returns:
            A variational encoder-decoder model.
        """
        # ENCODER
        encoder_input = Input(shape=(None, self.num_features), name="input")
        x = LSTM(
            self.hidden_units_1,  # 32
            return_sequences=False,
            stateful=False,
        )(encoder_input)
        encoder = tf.keras.Model(encoder_input, [x], name="encoder")

        # DECODER
        latent_input = Input(shape=(self.hidden_units_1), name="latent_input")
        x = RepeatVector(self.output_sequence_length)(latent_input)
        x = LSTM(
            self.hidden_units_2,
            return_sequences=True,
            stateful=False,
        )(x)
        z_mean = TimeDistributed(Dense(self.latent_dim), name="z_mean")(x)
        z_var = TimeDistributed(Dense(self.latent_dim), name="z_var")(x)
        z = SamplingLayer(name="sampling")([z_mean, z_var])
        x = TimeDistributed(Dense(self.num_features))(z)
        decoder_output = Activation("linear", name="output")(x)

        decoder = tf.keras.Model(latent_input, decoder_output, name="decoder")

        # Variational Encoder Decoder.
        config = VEDPredictorConfiguration(encoder, decoder)
        ved = VEDPredictor(config)

        # Compile the Variational Encoder Decoder.
        return self._compile_model(ved)

    def to_onnx(self, output_path: Union[str, Path]) -> None:
        """Save model to ONNX format."""
        try:
            import tf2onnx  # noqa: F401
        except ImportError as ie:
            raise ImportError(
                "tf2onnx must be installed to export to onnx"
            ) from ie

        self.model.save_weights("./lstm")
        tf.saved_model.save(self.model, "./")

        os.system(
            "python -m tf2onnx.convert --opset 15 --saved-model"
            f" ./ --output {output_path}/lstm.onnx"
        )

    def evaluate(
        self,
        x: Any,
        y: Any,
        evaluation_types: list[MetricType],
        y_scores: Optional[Any] = None,
        kwargs: Optional[Mapping[Any, Any]] = None,
    ) -> Union[dict[MetricType, float], dict[MetricType, list[float]]]:
        """Evaluates the trained model using supplied MetricType(s).

        Args:
            x: Evaluation data.
            y: Evaluation labels.
            evaluation_types: Type of model evaluation used for the predictor.
            y_scores: Preprocessed predictions for specific metrics (i.e.
                MetricType.ROC_AUC_SCORE)
            kwargs: Set of keyword arguments for evaluation.

        Raises:
            AttributeError if model is NOT already trained.

        Returns:
            A dictionary mapping evaluation metric types to the score(s)
            calculated for that metric type.
        """
        raise NotImplementedError

    def _check_validation_data(
        self, validation_data: tuple[np.ndarray, np.ndarray]
    ) -> None:
        """Validates the input validation data for training.

        Args:
            validation_data: The validation data input to model training.
        """
        x = validation_data[0]
        y = validation_data[1]

        if not (isinstance(x, np.ndarray)):
            raise TypeError("X_validation must be a numpy array")

        if not (isinstance(y, np.ndarray)):
            raise TypeError("y_validation must be a numpy array")

        if not (x.ndim == 3):
            raise ValueError(
                "X_validation must have 3 dimensions.\
                (examples, timesteps, features)"
            )
        if not (y.ndim == 3):
            raise ValueError(
                "y_validation must have 3 dimensions.\
                (examples, timesteps, features)"
            )

        if not ((x.shape[0] == y.shape[0]) and (x.shape[2] == y.shape[2])):
            raise ValueError(
                "X_validation and y_validation must match dimensions along axis\
                0 and 2"
            )

        if not (x.shape[2] == self.num_features):
            raise ValueError(
                "X_validation.shape[2] should be same size as the models\
                num_features"
            )

        if not (y.shape[1] == self.output_sequence_length):
            raise ValueError(
                "y_validation.shape[1] should be same size as the models\
                    output_sequence_length"
            )

    def _compile_model(self, model: tf.keras.Model) -> tf.keras.Model:
        """Compiles a keras model based on the loss data structure.

        Args:
            model: The keras model to compile.

        Returns:
            The compiled keras model.
        """
        # Compile the model based on loss data structure.
        if callable(self._configuration.loss):
            model.compile(
                loss=self._configuration.loss,
                optimizer=self._configuration.optimizer.value,
                metrics=[self._configuration.loss],
            )
        else:
            model.compile(
                loss=self._configuration.loss.value,
                optimizer=self._configuration.optimizer.value,
                metrics=[self._configuration.loss.value],
            )

        return model
