investigate: Assess redundancy in LSTM implementations, notebooks

There are different LSTM implementations in the PCM and then in torch-models. These different implementations are then used in different notebooks. We'd like to standardize on a single LSTM model implementation in torch-models that gets reused across all other CMs and notebooks.

For this first investigation, let's:

Gather a list of all the different LSTM implementations and notebooks in PCM and torch-models. (Future work may be to investigate other CMs.)
Identify the differences between the multiple different LSTM model implementations.
In particular, are there any good unique things in the PCM implementations that should be brought into the torch-models implementation to improve our common LSTM baseline? If we consolidate LSTMs, we don't want to lose any helpful features / capability in that process.
Assess the ease of using a single common LSTM baseline across the PCM LSTM notebooks. Will this be smooth? Are there nuances in a notebook or two that would require some custom LSTM implementation different from the baseline?
Write up the above so we can assess next steps involving code changes. Include links to the different implementations and links.
Bullet points with links are fine; does not need to be an extensive "report" - just enough where we can clearly decide what to bring in from each LSTM implementation into a common LSTM baseline (if anything not already in torch-models).
