# Copyright 2023 Lockheed Martin Corporation
# Lockheed Martin Proprietary Information

"""LSTM Encoder network structure."""

from __future__ import annotations

import torch
from torch import nn

from lm_ai_torch_models.activations import ACTIVATION_MAP, Activation


class LSTMEncoder(nn.Module):
    """An LSTM module that converts a sample into a vector representation."""

    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        n_layers: int,
        recurrent_dropout_rate: float = 0.0,  # Normal torch LSTM dropout rate.
        output_dropout_rate: float | None = None,
        activation: Activation | None = None,
    ) -> None:
        """Initializes an LSTM encoder.

        Args:
            input_dim: The number of input dimensions.
            hidden_dim: The number of dimensions in the hidden vector.
            output_dim: The number of dimensions in the output vector.
            n_layers: The number of LSTM layers.
            recurrent_dropout_rate: The fraction dropout for LSTM layers.
                This dropout is applied to all LSTM layers except the
                last one.
            output_dropout_rate: The fraction dropout for linear layer(s).
                This is applied to linear layer(s) after the initial
                LSTM layer(s).
            activation: The activation function to use after the LSTM.
                If None, no activation will be applied.
        """
        super().__init__()

        self.hidden_dim: int = hidden_dim
        self.output_dim: int = output_dim
        self.n_layers: int = n_layers
        minimum_dropout: float = 0.0
        self.recurrent_dropout_rate: float = (
            recurrent_dropout_rate if self.n_layers > 1 else minimum_dropout
        )

        self.lstm: nn.LSTM = nn.LSTM(
            input_size=input_dim,
            hidden_size=self.hidden_dim,
            num_layers=self.n_layers,
            batch_first=True,
            dropout=self.recurrent_dropout_rate,
        )
        self.fc: nn.Linear = nn.Linear(
            in_features=self.hidden_dim, out_features=output_dim
        )

        self.activation: nn.Module | None = None
        if activation is not None:
            self.activation = ACTIVATION_MAP[activation]()

        self.dropout: nn.Dropout | None = None
        if (
            output_dropout_rate is not None
            and output_dropout_rate > minimum_dropout
        ):
            self.dropout = nn.Dropout(output_dropout_rate)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Performs a forward pass through an LSTM encoder.

        Args:
            x: An input torch tensor.

        Returns:
            An output torch tensor.
        """
        hidden_state: torch.Tensor = torch.zeros(
            self.n_layers,
            x.size(0),
            self.hidden_dim,
            device=x.device,
        )
        cell_state: torch.Tensor = torch.zeros(
            self.n_layers,
            x.size(0),
            self.hidden_dim,
            device=x.device,
        )
        lstm_out: torch.Tensor = self.lstm(x, (hidden_state, cell_state))[0]

        out: torch.Tensor = lstm_out[:, -1]

        if self.activation is not None:
            out = self.activation(out)

        out = self.fc(out)
        if self.dropout is not None:
            out = self.dropout(out)

        return out


class LSTMGenerator(nn.Module):
    """An LSTM module that converts a vector representation into a sample."""

    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        n_timesteps: int,
        n_layers: int,
        recurrent_dropout_rate: float = 0.0,  # Normal torch LSTM dropout rate.
        output_dropout_rate: float | None = None,
    ) -> None:
        """Initializes an LSTM generator.

        Args:
            input_dim: The number of input (latent) dimensions.
            hidden_dim: The number of dimensions in the hidden vector.
            output_dim: The number of features in the output.
            n_timesteps: The number of elements in each time sequence.
            n_layers: The number of LSTM layers.
            recurrent_dropout_rate: The fraction dropout for LSTM layers.
                This dropout is applied to all LSTM layers except the
                last one.
            output_dropout_rate: The fraction dropout for linear layer(s).
                This is applied to linear layer(s) after the initial
                LSTM layer(s).
        """
        super().__init__()
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        self.n_timesteps = n_timesteps
        minimum_dropout: float = 0.0
        self.recurrent_dropout_rate: float = (
            recurrent_dropout_rate if self.n_layers > 1 else minimum_dropout
        )

        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=self.hidden_dim,
            num_layers=self.n_layers,
            batch_first=True,
            dropout=self.recurrent_dropout_rate,
        )
        self.fc = nn.Linear(
            in_features=self.hidden_dim, out_features=self.output_dim
        )

        self.dropout: nn.Dropout | None = None
        if (
            output_dropout_rate is not None
            and output_dropout_rate > minimum_dropout
        ):
            self.dropout = nn.Dropout(output_dropout_rate)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Performs a forward pass through an LSTM generator.

        Args:
            x: An input torch tensor.

        Returns:
            An output torch tensor.
        """
        repeated: torch.Tensor = x.unsqueeze(1).repeat(1, self.n_timesteps, 1)
        lstm_out: torch.Tensor = self.lstm(repeated)[0]

        out: torch.Tensor = lstm_out.view(-1, self.n_timesteps, self.hidden_dim)
        out = self.fc(out)

        if self.dropout is not None:
            out = self.dropout(out)

        return out
