import streamlit as st
import plotly.graph_objects as go
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from pathlib import Path
import numpy as np

st.set_page_config(layout="wide")
st.title("Anomaly Detection Dashboard (Multi-CSV)")

# ---- CONFIG ----
DATA_DIR = Path("data/V/Report")   # directory containing multiple CSVs
REQUIRED_BASE = {"te_id","script_file_name","passes","fails","blocks","unattempts","simulation_host"}
OPTIONAL = {"total_init_time","t_v","s_number","build","capability"}

# Map common names -> canonical names used in app
RENAME_MAP = {
    "te_id": "Test_Run_ID",
    "script_file_name": "Script_File",
    "passes": "P",
    "fails": "F",
    "blocks": "B",
    "unattempts": "U",
    "t_v": "V",
    "capability": "Capability",
    "simulation_host": "Simulation_Host",
    "build": "Build",
    "s_number": "s_number",
    "total_init_time": "total_init_time",
}

CONTINUOUS_LABELS = {"FR", "Num_Tests", "total_init_time"}
LABEL_DICT = {
    "Failure Rate": "FR",
    "Total Tests": "Num_Tests",
    "Total Time": "total_init_time",
    "Simulation Host": "Simulation_Host",
    "Simulation Node": "Simulation_Node",
    "Capability": "Capability",
    "Script File": "Script_File",
    "V": "V",
    "S Number": "s_number",
    "Build": "Build",
}

@st.cache_data
def list_csvs(data_dir: Path):
    return sorted([p for p in data_dir.glob("*.csv") if p.is_file()])

@st.cache_data
def load_csv(path: Path) -> pd.DataFrame:
    df = pd.read_csv(path)
    # normalize column names: strip spaces, lower for matching, keep original casing later
    df.columns = df.columns.str.strip().str.lower()
    missing = REQUIRED_BASE - set(df.columns)
    if missing:
        raise ValueError(f"{path.name}: missing required columns {missing}")
    # keep only needed columns if present
    cols_to_keep = list(REQUIRED_BASE | OPTIONAL)
    cols_to_keep = [c for c in cols_to_keep if c in df.columns]
    df = df[cols_to_keep].copy()
    # rename to canonical names the rest of the app expects
    df.rename(columns=RENAME_MAP, inplace=True)
    # minimal type cleaning
    for c in ["P","F","B","U","total_init_time"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    # ensure strings
    if "Simulation_Host" in df.columns:
        df["Simulation_Host"] = df["Simulation_Host"].astype(str)
    if "s_number" in df.columns:
        df["s_number"] = df["s_number"].astype(str)
    return df

def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # drop rows without a host (as in original)
    if "Simulation_Host" in df.columns:
        df = df.dropna(subset=["Simulation_Host"])
    # simulation node from host
    if "Simulation_Host" in df.columns:
        df["Simulation_Node"] = df["Simulation_Host"].astype(str).str.split("-", n=1).str[0]
    # totals & failure rate
    for c in ["P","F","B","U"]:
        if c not in df.columns:
            df[c] = 0
    df["Num_Tests"] = df[["P","F","B","U"]].sum(axis=1)
    # FR = F / total; if denom==0 set to 1 (as in original)
    denom = df["Num_Tests"].replace(0, np.nan)
    df["FR"] = (df["F"] / denom).fillna(1.0)
    # time feature default
    if "total_init_time" not in df.columns:
        df["total_init_time"] = 0.0
    return df

def build_features(df: pd.DataFrame) -> pd.DataFrame:
    """Return the numeric matrix used for PCA/scoring."""
    feats = ["P","F","B","U","total_init_time","FR","Num_Tests"]
    return df[feats].fillna(0.0).astype(float)

def fit_pca_scaled(X: np.ndarray, n_components: int = 3):
    scaler = StandardScaler()
    Xz = scaler.fit_transform(X)
    k = max(1, min(n_components, Xz.shape[0], Xz.shape[1]))
    pca = PCA(n_components=k)
    PCs = pca.fit_transform(Xz)
    return scaler, pca, Xz, PCs

def compute_anomaly_scores(PCs: np.ndarray) -> np.ndarray:
    # keep original logic but robust to zero variance
    mean = PCs.mean(axis=0)
    std = PCs.std(axis=0)
    m = std.max()
    if m == 0:
        m = 1.0
    scores = np.linalg.norm(PCs - mean, axis=1) / m
    return scores

def plot_for_df(df: pd.DataFrame, PCs: np.ndarray, selected_label_name: str) -> go.Figure:
    selected_label = LABEL_DICT[selected_label_name]
    scores = compute_anomaly_scores(PCs)

    # choose 3D or 2D based on available components
    is_3d = PCs.shape[1] >= 3

    # shared hover base
    base_cols = ["Test_Run_ID","FR","Num_Tests","total_init_time"]
    for c in base_cols:
        if c not in df.columns:
            df[c] = np.nan

    if selected_label in CONTINUOUS_LABELS:
        val = df[selected_label].values
        if selected_label == "FR":
            colorscale = [[0, 'green'], [1, 'red']]
            cmin, cmax = 0, 1
            color_vals = val
        else:
            vmax = np.nanmax(val) if np.isfinite(val).any() else 1.0
            vmax = vmax if vmax > 0 else 1.0
            color_vals = np.sqrt(np.clip(val, 0, None) / vmax)
            colorscale = [[0, 'blue'], [1, 'red']]
            cmin, cmax = 0, 1

        hovertext = [
            f"Test Run ID: {trid},<br>Failure Rate: {fr:.3f},<br>Total Tests: {nt},<br>Total Time: {tt},<br>Anomaly Score: {s:.3f}"
            for trid, fr, nt, tt, s in zip(df["Test_Run_ID"], df["FR"], df["Num_Tests"], df["total_init_time"], scores)
        ]

        marker = dict(color=color_vals, cmin=cmin, cmax=cmax, colorscale=colorscale, showscale=True)

        if is_3d:
            fig = go.Figure(go.Scatter3d(
                x=PCs[:,0], y=PCs[:,1], z=PCs[:,2],
                mode="markers", hovertext=hovertext, hoverinfo="text", marker=marker
            ))
            fig.update_layout(scene=dict(xaxis_title='PC1', yaxis_title='PC2', zaxis_title='PC3'))
        else:
            fig = go.Figure(go.Scatter(
                x=PCs[:,0], y=PCs[:,1],
                mode="markers", hovertext=hovertext, hoverinfo="text", marker=marker
            ))
            fig.update_layout(xaxis_title='PC1', yaxis_title='PC2')

    else:
        # categorical split
        vals = df[selected_label]
        uniq = vals.dropna().unique()
        if is_3d:
            fig = go.Figure()
            for label in uniq:
                idx = vals == label
                hovertext = [
                    f"Test Run ID: {trid},<br>Failure Rate: {fr:.3f},<br>Total Tests: {nt},<br>{selected_label}: {label},<br>Anomaly Score: {s:.3f}"
                    for trid, fr, nt, s in zip(df.loc[idx,"Test_Run_ID"], df.loc[idx,"FR"], df.loc[idx,"Num_Tests"], compute_anomaly_scores(PCs[idx]))
                ]
                fig.add_trace(go.Scatter3d(
                    x=PCs[idx,0], y=PCs[idx,1], z=PCs[idx,2],
                    mode="markers", name=str(label), hovertext=hovertext, hoverinfo="text"
                ))
            fig.update_layout(scene=dict(xaxis_title='PC1', yaxis_title='PC2', zaxis_title='PC3'))
        else:
            fig = go.Figure()
            for label in uniq:
                idx = vals == label
                hovertext = [
                    f"Test Run ID: {trid},<br>Failure Rate: {fr:.3f},<br>Total Tests: {nt},<br>{selected_label}: {label},<br>Anomaly Score: {s:.3f}"
                    for trid, fr, nt, s in zip(df.loc[idx,"Test_Run_ID"], df.loc[idx,"FR"], df.loc[idx,"Num_Tests"], compute_anomaly_scores(PCs[idx]))
                ]
                fig.add_trace(go.Scatter(
                    x=PCs[idx,0], y=PCs[idx,1],
                    mode="markers", name=str(label), hovertext=hovertext, hoverinfo="text"
                ))
            fig.update_layout(xaxis_title='PC1', yaxis_title='PC2')

    fig.update_layout(width=1600, height=800, margin=dict(l=10,r=10,t=50,b=10))
    return fig

# ---- UI controls (apply to all files) ----
st.sidebar.header("Controls")
label_choice = st.sidebar.selectbox("Color/Group by", list(LABEL_DICT.keys()))
show_top = st.sidebar.number_input("Show Top-N anomalies table per file (0 to hide)", min_value=0, max_value=1000, value=10, step=1)

# ---- Load all CSVs ----
csv_paths = list_csvs(DATA_DIR)
if not csv_paths:
    st.warning(f"No CSV files found in: {DATA_DIR.resolve()}")
    st.stop()

tabs = st.tabs([p.name for p in csv_paths])

for tab, path in zip(tabs, csv_paths):
    with tab:
        st.subheader(path.name)
        try:
            raw = load_csv(path)
            df = engineer_features(raw)
            X = build_features(df)
            if X.shape[0] < 3:
                st.info(f"Not enough rows to compute 3D PCA (rows={X.shape[0]}). Showing 2D if possible.")
            scaler, pca, Xz, PCs = fit_pca_scaled(X.to_numpy(), n_components=3)
            # anomaly scores (same logic across files)
            scores = compute_anomaly_scores(PCs)
            df["anomaly_score"] = scores

            fig = plot_for_df(df, PCs, label_choice)
            st.plotly_chart(fig, use_container_width=True)

            if show_top > 0:
                top = df.sort_values("anomaly_score", ascending=False).head(show_top)
                st.markdown("**Top anomalies**")
                st.dataframe(top[["Test_Run_ID","FR","Num_Tests","total_init_time","anomaly_score","Simulation_Host","Simulation_Node","Capability","Script_File","V","s_number","Build"]].fillna(""), use_container_width=True)

        except Exception as e:
            st.error(f"Failed to process {path.name}: {e}")
