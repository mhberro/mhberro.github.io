def build_representation_matrix(df: pd.DataFrame) -> tuple[pd.DataFrame, list[str]]:
    """
    Build a 0/1 representation matrix for FEATURES_FOR_ANALYSIS.

    - For categorical features:
        One-hot encode values => columns like 'Capability=A'.
    - For numeric features:
        Bin into quartiles over *all* rows => columns like 'total_init_time:Q1'...'Q4'.

    Returns:
        R      : DataFrame (same index as df), all columns are 0/1.
        labels : list of column names in R.
    """
    parts = []
    labels: list[str] = []

    for feat in FEATURES_FOR_ANALYSIS:
        if feat not in df.columns:
            continue
        s = df[feat]

        # CATEGORICAL: treat as strings and one-hot encode
        if not np.issubdtype(s.dtype, np.number):
            vals = s.astype(str)
            dummies = pd.get_dummies(vals, prefix=f"{feat}=")
            if dummies.empty:
                continue
            # Clean up labels to 'feat=value'
            dummies.columns = [col.replace("==", "=") for col in dummies.columns]
            parts.append(dummies.astype(int))
            labels.extend(dummies.columns.tolist())

        else:
            # NUMERIC: bin into quartiles and one-hot encode bins
            s_num = pd.to_numeric(s, errors="coerce")
            if s_num.notna().sum() < 4:
                continue
            try:
                bins = pd.qcut(s_num, 4, labels=["Q1", "Q2", "Q3", "Q4"], duplicates="drop")
            except Exception:
                continue
            dummies = pd.get_dummies(bins, prefix=f"{feat}:")
            if dummies.empty:
                continue
            # Clean labels like 'feat::Q1' -> 'feat:Q1'
            dummies.columns = [col.replace("::", ":") for col in dummies.columns]
            parts.append(dummies.astype(int))
            labels.extend(dummies.columns.tolist())

    if not parts:
        return pd.DataFrame(index=df.index), []

    R = pd.concat(parts, axis=1).reindex(index=df.index, fill_value=0)
    return R, labels





=========================================================================================================================================================================
=========================================================================================================================================================================
def fr_correlation_heatmap(df: pd.DataFrame) -> go.Figure:
    """
    Representation heatmap based on a 0/1 matrix:

    For each feature/value/bin column:
      - Consider only rows where that column == 1 AND FR segment is High or Low.
      - High_FR% = 100 * (# High_FR rows with col=1) / (# High_FR + Low_FR rows with col=1)
      - Low_FR%  = 100 * (# Low_FR  rows with col=1) / (same denom)
      => High_FR% + Low_FR% = 100% for each column.

    Shows top-10 columns by High_FR%.
    """
    fig = go.Figure()
    seg = make_fr_segments(df)  # "Low FR" / "Mid FR" / "High FR"
    if seg is None or seg.isna().all():
        fig.update_layout(
            title="FR representation (segments unavailable)",
            height=200, margin=dict(l=10, r=10, t=40, b=40),
        )
        return fig

    # Build 0/1 representation matrix
    R, rep_labels = build_representation_matrix(df)
    if R.empty:
        fig.update_layout(
            title="FR representation (no valid features in FEATURES_FOR_ANALYSIS)",
            height=200, margin=dict(l=10, r=10, t=40, b=40),
        )
        return fig

    # Restrict to High/Low rows only (exclude Mid FR from denominators)
    mask_hl = seg.isin(["High FR", "Low FR"])
    if not mask_hl.any():
        fig.update_layout(
            title="FR representation (no High/Low FR rows)",
            height=200, margin=dict(l=10, r=10, t=40, b=40),
        )
        return fig

    seg_hl = seg[mask_hl]
    R_hl = R.loc[mask_hl]

    high_mask = seg_hl == "High FR"
    low_mask = seg_hl == "Low FR"

    # Counts per column (vectorized)
    high_counts = R_hl[high_mask].sum(axis=0).astype(float)
    low_counts  = R_hl[low_mask].sum(axis=0).astype(float)
    denom = high_counts + low_counts

    # Only keep columns that occur at least once in High or Low
    valid = denom > 0
    if not valid.any():
        fig.update_layout(
            title="FR representation (no values present in High/Low FR)",
            height=200, margin=dict(l=10, r=10, t=40, b=40),
        )
        return fig

    high_pct = pd.Series(0.0, index=denom.index)
    low_pct  = pd.Series(0.0, index=denom.index)
    high_pct[valid] = 100.0 * high_counts[valid] / denom[valid]
    low_pct[valid]  = 100.0 * low_counts[valid]  / denom[valid]

    # Normalize each column to guard against tiny numerical drift
    col_sums = high_pct + low_pct
    nz = col_sums > 0
    high_pct[nz] = 100.0 * high_pct[nz] / col_sums[nz]
    low_pct[nz]  = 100.0 * low_pct[nz]  / col_sums[nz]

    mat = pd.DataFrame({"High_FR": high_pct, "Low_FR": low_pct})

    # Pick top-10 columns by High_FR percentage
    top_idx = mat["High_FR"].sort_values(ascending=False).head(10).index
    mat_top = mat.loc[top_idx].T  # rows: High_FR, Low_FR

    fig.add_trace(go.Heatmap(
        z=mat_top.values,
        x=mat_top.columns.tolist(),
        y=mat_top.index.tolist(),
        text=np.round(mat_top.values, 1).astype(str) + "%",
        texttemplate="%{text}",
        zmin=0, zmax=100,
        colorscale="YlOrRd",
        colorbar=dict(title="% of rows with this value/bin\nthat are High vs Low"),
    ))
    fig.update_layout(
        title="Feature/Value Representation within High-FR vs Low-FR (per value/bin)",
        height=320,
        margin=dict(l=10, r=10, t=40, b=10),
    )
    return fig




=========================================================================================================================================================================
=========================================================================================================================================================================



=========================================================================================================================================================================
=========================================================================================================================================================================

=========================================================================================================================================================================
=========================================================================================================================================================================

=========================================================================================================================================================================
=========================================================================================================================================================================




=========================================================================================================================================================================
=========================================================================================================================================================================

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# Background colors: 0 = UNTAGGED (green), 1 = TAGGED (red)
cmap_bg = ListedColormap(["#81c784", "#e57373"])  # light green, light red

# Point colors (slightly darker)
point_colors = {
    "UNTAGGED": "#388e3c",  # darker green
    "TAGGED":   "#c62828",  # darker red
}

# Build a grid over the feature space
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1

xx, yy = np.meshgrid(
    np.linspace(x_min, x_max, 300),
    np.linspace(y_min, y_max, 300),
)

grid = np.c_[xx.ravel(), yy.ravel()]
Z = clf_2d.predict(grid).reshape(xx.shape)  # 0 or 1 for each grid point

plt.figure(figsize=(8, 6))

# Decision regions
plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_bg)

# Actual samples on top
for label, color in point_colors.items():
    mask = df["outcome"] == label
    plt.scatter(
        df.loc[mask, "target_distance"],
        df.loc[mask, "closing_speed"],
        s=20,
        c=color,
        label=label,
        edgecolor="k",
        linewidth=0.3,
        alpha=0.8,
    )

plt.xlabel("target_distance (ECEF Euclidean, e.g. meters)")
plt.ylabel("closing_speed (it_speed - runner_speed)")
plt.title("Decision tree boundary for TAGGED vs UNTAGGED")
plt.legend()
plt.tight_layout()
plt.show()


=========================================================================================================================================================================
=========================================================================================================================================================================
def _render_table_and_plot(
    df: pd.DataFrame,
    PCs: np.ndarray,
    scores01: np.ndarray,
    raw_scores: np.ndarray,
    method_label: str,
    key_prefix: str,
    pca,  # global PCA (for loadings)
):
    """
    Renders three visual perspectives for a given anomaly method:

      1) Storyboard:
         - Top anomalies table with row selection
         - PCA scatter with optional highlight
         - PCA loading arrows (global + segment-anchored)
         - Segment Comparison & FR correlation
         - Per-selection 'Why this run?' bar chart

      2) Cluster Heatmap:
         - Cluster-vs-feature heatmap in z-score space

      3) Feature Network:
         - Small network of top anomaly-driving features
    """
    df = df.copy()
    df["anomaly_score"] = scores01
    df["raw_anomaly_score"] = raw_scores

    st.markdown(f"### {method_label}")

    tab_story, tab_cluster, tab_network = st.tabs(
        ["Storyboard", "Cluster Heatmap", "Feature Network"]
    )

    # === 1) STORYBOARD TAB ===
    with tab_story:
        # --- Top anomalies: select a row to highlight on the plot ---
        highlight_pos = None
        if show_top > 0:
            cols_keep = [
                "Test_Run_ID",
                "FR",
                "Num_Tests",
                "total_init_time",
                "anomaly_score",
                "raw_anomaly_score",
                "Simulation_Host",
                "Simulation_Node",
                "Capability",
                "Script_File",
                "V",
                "s_number",
                "Build",
            ]
            cols_keep = [c for c in cols_keep if c in df.columns]

            top = df.sort_values("anomaly_score", ascending=False).head(show_top)

            st.markdown("**Top anomalies**")
            editable = top[cols_keep].copy()
            editable.insert(0, "Select", False)

            edited = st.data_editor(
                editable,
                use_container_width=True,
                num_rows="fixed",
                disabled={c: True for c in editable.columns if c != "Select"},
                key=f"{key_prefix}-table",
            )

            selected_original_idxs = edited.index[edited["Select"]].tolist()
            if selected_original_idxs:
                pos = df.index.get_indexer([selected_original_idxs[0]])
                if len(pos) and pos[0] != -1:
                    highlight_pos = int(pos[0])

        # --- PCA plot with optional highlight ---
        fig = plot_for_df(df, PCs, label_choice, highlight_pos=highlight_pos)

        # === Driver arrows overlays (global + low/high FR anchors) ===
        if show_biplot and PCs.shape[1] >= 2 and pca is not None:
            # 1) Global arrows from origin
            try:
                add_pca_loadings_3d(fig, pca, PCA_FEATURES, scale=2.0)
            except Exception:
                pass

            # 2) Segment-anchored arrows near Low-FR and High-FR bands
            try:
                seg = make_fr_segments(df)  # "Low FR" / "Mid FR" / "High FR"
                # Low-FR centroid
                low_mask = (seg == "Low FR").values
                if np.any(low_mask):
                    low_anchor = PCs[low_mask].mean(axis=0)
                    add_biplot_arrows_at_anchor_3d(
                        fig,
                        pca,
                        PCA_FEATURES,
                        low_anchor,
                        scale=1.2,
                        color="green",
                        name_prefix="Low FR ",
                        showlegend=False,
                    )
                # High-FR centroid
                high_mask = (seg == "High FR").values
                if np.any(high_mask):
                    high_anchor = PCs[high_mask].mean(axis=0)
                    add_biplot_arrows_at_anchor_3d(
                        fig,
                        pca,
                        PCA_FEATURES,
                        high_anchor,
                        scale=1.2,
                        color="red",
                        name_prefix="High FR ",
                        showlegend=False,
                    )
            except Exception:
                pass

        st.plotly_chart(fig, use_container_width=True, key=f"{key_prefix}-plot")

        # --- Segment Comparison + FR Correlation ---
        if show_segment_compare:
            c1, c2 = st.columns([2, 1])
            with c1:
                st.markdown("#### Segment Comparison: Low vs High FR")
                st.plotly_chart(
                    segment_diff_bar(df),
                    use_container_width=True,
                    key=f"{key_prefix}-segdiff",
                )
            with c2:
                st.markdown("#### FR Correlation")
                st.plotly_chart(
                    fr_correlation_heatmap(df),
                    use_container_width=True,
                    key=f"{key_prefix}-frcorr",
                )

        # --- Per-selection “Why this run?” ---
        if show_selection_why and highlight_pos is not None:
            st.markdown("#### Why this run?")
            st.plotly_chart(
                per_point_contribution_bar(df, highlight_pos),
                use_container_width=True,
                key=f"{key_prefix}-whybar",
            )

    # === 2) CLUSTER HEATMAP TAB ===
    with tab_cluster:
        st.markdown("#### Cluster-level feature patterns")
        st.plotly_chart(
            cluster_profile_heatmap(df, PCs, scores01),
            use_container_width=True,
            key=f"{key_prefix}-clusterheat",
        )

    # === 3) FEATURE NETWORK TAB ===
    with tab_network:
        # Use the most anomalous subset to focus the network
        n_sub = max(20, int(0.2 * len(df)))
        df_anom = df.sort_values("anomaly_score", ascending=False).head(n_sub)
        st.markdown("#### Relationships among top anomaly-driving features")
        st.plotly_chart(
            feature_influence_network_figure(df_anom),
            use_container_width=True,
            key=f"{key_prefix}-featnet",
        )


