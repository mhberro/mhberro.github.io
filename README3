If I'm understanding things correctly, I think we'd want to:
Keep the torch-training nn.Module implementation as our core common LSTM implementation, and eventually discard all other implementations of nn.Modules (basically anything actually creating the layers and defining the forward() pass).
Have 1 (?) place where we implement the NNModel with fit() etc, probably in PCM for regression-style outputs. (Maybe we eventually want a separate one in CCM for classification-style outputs?)
And in practice, maybe that might be the following steps?
In PCM mlearn/lstm/, could we get rid of architecture.py and/or model.py, and instead have the lstm.py NNModel reference the torch-training architecture.py and lstm.py nn.Module under the hood instead? Optionally with something in PCM to put some regression on top.
Update one notebook in PCM to use this new stack, which may or may not require code changes in the notebook. It's possible that any existing references to the PCM lstm NNModel might "just work" even if we change things under the hood as mentioned above right?
I might be missing something though, please correct me if I'm wrong
