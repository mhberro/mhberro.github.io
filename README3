F-35 anomaly detection

def add_pca_loadings_3d(
    fig: go.Figure,
    pca,
    feature_names: list[str],
    scale: float = 2.0,
):
    """
    Overlay PCA loading arrows (global biplot) for first 2–3 PCs.

    feature_names must correspond to the numeric features used to fit the PCA.
    """
    loadings = getattr(pca, "components_", None)
    if loadings is None:
        return fig
    loadings = loadings.T  # shape: (n_features, n_components)
    k = min(3, loadings.shape[1])
    if k < 2:
        return fig

    # Try to align by feature_names_in_ if available
    base_names = list(getattr(pca, "feature_names_in_", []))
    indices = []
    labels = []
    if base_names:
        for fname in feature_names:
            if fname in base_names:
                i = base_names.index(fname)
                if i < loadings.shape[0]:
                    indices.append(i)
                    labels.append(fname)
    else:
        # Fallback: assume same order as feature_names
        for i, fname in enumerate(feature_names):
            if i < loadings.shape[0]:
                indices.append(i)
                labels.append(fname)

    for idx, fname in zip(indices, labels):
        vec = loadings[idx, :k] * scale
        if k >= 3:
            x, y, z = [0, float(vec[0])], [0, float(vec[1])], [0, float(vec[2])]
            fig.add_trace(
                go.Scatter3d(
                    x=x,
                    y=y,
                    z=z,
                    mode="lines",
                    line=dict(width=5),
                    name=f"→ {fname}",
                    showlegend=False,
                )
            )
            fig.add_trace(
                go.Scatter3d(
                    x=[x[-1]],
                    y=[y[-1]],
                    z=[z[-1]],
                    mode="text",
                    text=[fname],
                    showlegend=False,
                )
            )
        else:
            x, y = [0, float(vec[0])], [0, float(vec[1])]
            fig.add_trace(
                go.Scatter(
                    x=x,
                    y=y,
                    mode="lines+text",
                    text=[None, fname],
                    textposition="top center",
                    showlegend=False,
                )
            )
    return fig





=========================================================================================================================================================================
=========================================================================================================================================================================




In 2026, I plan to bridge the gap between experimental AI research and the architectural rigor required for mission-critical autonomous systems. I will continue advancing our core ML and computer vision algorithms, but with an increased focus on developing formal implementation requirements that ensure system-level reliability. By interfacing more deeply with the simulation and modeling teams, I aim to optimize our prototypes for deployment in complex, multi-domain environments. My goal is to evolve from an algorithm-specific expert into a lead who oversees how AI components integrate securely within larger mission systems. This includes applying the latest research in model robustness and explainability to meet the high standards of defense-grade autonomy. Ultimately, I want to drive development cycles that prioritize both high-end performance and the long-term safety of our autonomous platforms.
Building on my 2025 transition from foundational software practices to hands-on model development, I aim to grow in 2026 by evolving from an individual contributor into a cross-functional technical lead within the autonomous systems space. My goal is to bridge the gap between pure AI research and mission-ready prototypes by taking ownership of the end-to-end development cycle for a major project component. I plan to leverage my EDA and modeling experience to drive advancements in perception and motion planning, ensuring our algorithms are not only state-of-the-art but also optimized for LM platforms. By deepening collaborations with simulation and testing teams, I will ensure our research meets rigorous implementation requirements.
=========================================================================================================================================================================
=========================================================================================================================================================================



=========================================================================================================================================================================
=========================================================================================================================================================================

=========================================================================================================================================================================
=========================================================================================================================================================================
kinematic-classifier

# conftest.py (alternative)
import pytest, shutil
from pathlib import Path

@pytest.fixture()
def repo_data_files_with_fakes(tmp_path, monkeypatch):
    root = Path(__file__).resolve().parents[1]
    fake_src = root / "data" / "fake_data_files"
    real_dst = root / "my_pkg" / "data" / "data_files"

    backups = []
    real_dst.mkdir(parents=True, exist_ok=True)

    # backup any existing files, then copy fakes
    for f in real_dst.glob("*"):
        b = tmp_path / f.name
        shutil.copy2(f, b)
        backups.append((f, b))
    for f in fake_src.iterdir():
        if f.is_file():
            shutil.copy2(f, real_dst / f.name)

    yield real_dst  # tests run with fakes in place

    # restore previous contents
    for f in real_dst.glob("*"):
        f.unlink()
    for dst, backup in backups:
        shutil.copy2(backup, dst)

=========================================================================================================================================================================
=========================================================================================================================================================================




=========================================================================================================================================================================
=========================================================================================================================================================================
PG-35: Susceptibility Modeling

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker  # Import ticker for custom labels
import seaborn as sns

def plot_dogfight_final_units(df_analyzed):
    plt.figure(figsize=(12, 12))
    ax = plt.subplot(111, projection='polar')
    
    # 1. Orientation Setup
    ax.set_theta_zero_location("N") 
    ax.set_theta_direction(-1)
    
    # ---------------------------------------------------------
    # FORMATTING THE DISTANCE UNITS (New Addition)
    # ---------------------------------------------------------
    # This adds ' m' to every number on the radial circles
    ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.0f m'))
    
    # Move the distance numbers to the 45-degree line so they don't 
    # overlap with the Head/Tail labels
    ax.set_rlabel_position(45)
    
    # ---------------------------------------------------------
    # DATA PREPARATION
    # ---------------------------------------------------------
    def get_target_speed(row):
        return row['runner_speed'][row['it_runner_index']]

    df_analyzed['target_speed_val'] = df_analyzed.apply(get_target_speed, axis=1)
    
    avg_it_speed = df_analyzed['it_speed'].mean()
    avg_runner_speed = df_analyzed['target_speed_val'].mean()
    
    groups = [
        {'label': 'Untag (Escaped)', 'color': 'blue', 'alpha': 0.2, 'data': df_analyzed[df_analyzed['outcome'] == 'untag']},
        {'label': 'Tag (Captured)',  'color': 'red',  'alpha': 0.7, 'data': df_analyzed[df_analyzed['outcome'] == 'tag']}
    ]
    
    # ---------------------------------------------------------
    # PLOTTING
    # ---------------------------------------------------------
    max_radius_found = 0
    
    for group in groups:
        subset = group['data']
        if subset.empty: continue
            
        theta = np.deg2rad(subset['relative_approach_angle'])
        
        # Check for distance column
        if 'distance_to_target' in subset.columns:
            r = subset['distance_to_target']
        else:
            r = np.random.uniform(5, 20, len(subset)) # Using larger numbers to show 'm' clearly
            
        if r.max() > max_radius_found:
            max_radius_found = r.max()

        # Vector Math for Headings
        target_headings = np.array([row['runner_heading'][row['it_runner_index']] 
                                    for _, row in subset.iterrows()])
        it_headings = subset['it_heading'].values
        rel_heading_rad = np.deg2rad(it_headings - target_headings)
        
        angle_diff = rel_heading_rad - theta
        u = np.cos(angle_diff)
        v = np.sin(angle_diff)
        
        ax.quiver(theta, r, u, v,
                  color=group['color'],
                  alpha=group['alpha'],
                  scale=30,
                  pivot='mid',
                  headwidth=3,
                  headlength=4,
                  label=group['label'])

    # ---------------------------------------------------------
    # LABELS
    # ---------------------------------------------------------
    
    # Set limit slightly higher than max data to fit everything
    ax.set_ylim(0, max_radius_found * 1.15)
    r_label_pos = max_radius_found * 1.25 
    
    plt.text(0, r_label_pos, "HEAD\n(Front)", ha='center', va='bottom', fontweight='bold', fontsize=11)
    plt.text(np.pi, r_label_pos, "TAIL\n(Rear)", ha='center', va='top', fontweight='bold', fontsize=11)
    plt.text(np.pi/2, r_label_pos, "Right Flank", ha='left', va='center', fontsize=10)
    plt.text(-np.pi/2, r_label_pos, "Left Flank", ha='right', va='center', fontsize=10)
    
    ax.plot(0, 0, marker='^', markersize=15, color='black', label='Target Runner')

    plt.title("Drone Dogfight Analysis: Approach Vectors", fontsize=16, y=1.15, fontweight='bold')
    
    # Speed Stats Box
    speed_text = (f"STATS:\n"
                  f"Avg It Speed: {avg_it_speed:.1f} m/s\n"
                  f"Avg Target Speed: {avg_runner_speed:.1f} m/s\n"
                  f"Delta: {'+' if avg_it_speed > avg_runner_speed else ''}{avg_it_speed - avg_runner_speed:.1f} m/s")
    
    plt.figtext(0.02, 0.95, speed_text, ha='left', va='top', fontsize=10, 
                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round,pad=0.5'))

    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    plt.tight_layout()
    plt.show()

# plot_dogfight_final_units(df_analyzed)
=========================================================================================================================================================================
=========================================================================================================================================================================



=========================================================================================================================================================================
=========================================================================================================================================================================



=========================================================================================================================================================================
=========================================================================================================================================================================


=========================================================================================================================================================================
=========================================================================================================================================================================


=========================================================================================================================================================================
=========================================================================================================================================================================


=========================================================================================================================================================================
=========================================================================================================================================================================
Extras

- Anomaly Detection
def _render_table_and_plot(
    df: pd.DataFrame,
    PCs: np.ndarray,
    scores01: np.ndarray,
    raw_scores: np.ndarray,
    method_label: str,
    key_prefix: str,
    pca,  # global PCA (for loadings)
):
    """
    Renders three visual perspectives for a given anomaly method:

      1) Storyboard:
         - Top anomalies table with row selection
         - PCA scatter with optional highlight
         - PCA loading arrows (global + segment-anchored)
         - Segment Comparison & FR correlation
         - Per-selection 'Why this run?' bar chart

      2) Cluster Heatmap:
         - Cluster-vs-feature heatmap in z-score space

      3) Feature Network:
         - Small network of top anomaly-driving features
    """
    df = df.copy()
    df["anomaly_score"] = scores01
    df["raw_anomaly_score"] = raw_scores

    st.markdown(f"### {method_label}")

    tab_story, tab_cluster, tab_network = st.tabs(
        ["Storyboard", "Cluster Heatmap", "Feature Network"]
    )

    # === 1) STORYBOARD TAB ===
    with tab_story:
        # --- Top anomalies: select a row to highlight on the plot ---
        highlight_pos = None
        if show_top > 0:
            cols_keep = [
                "Test_Run_ID",
                "FR",
                "Num_Tests",
                "total_init_time",
                "anomaly_score",
                "raw_anomaly_score",
                "Simulation_Host",
                "Simulation_Node",
                "Capability",
                "Script_File",
                "V",
                "s_number",
                "Build",
            ]
            cols_keep = [c for c in cols_keep if c in df.columns]

            top = df.sort_values("anomaly_score", ascending=False).head(show_top)

            st.markdown("**Top anomalies**")
            editable = top[cols_keep].copy()
            editable.insert(0, "Select", False)

            edited = st.data_editor(
                editable,
                use_container_width=True,
                num_rows="fixed",
                disabled={c: True for c in editable.columns if c != "Select"},
                key=f"{key_prefix}-table",
            )

            selected_original_idxs = edited.index[edited["Select"]].tolist()
            if selected_original_idxs:
                pos = df.index.get_indexer([selected_original_idxs[0]])
                if len(pos) and pos[0] != -1:
                    highlight_pos = int(pos[0])

        # --- PCA plot with optional highlight ---
        fig = plot_for_df(df, PCs, label_choice, highlight_pos=highlight_pos)

        # === Driver arrows overlays (global + low/high FR anchors) ===
        if show_biplot and PCs.shape[1] >= 2 and pca is not None:
            # 1) Global arrows from origin
            try:
                add_pca_loadings_3d(fig, pca, PCA_FEATURES, scale=2.0)
            except Exception:
                pass

            # 2) Segment-anchored arrows near Low-FR and High-FR bands
            try:
                seg = make_fr_segments(df)  # "Low FR" / "Mid FR" / "High FR"
                # Low-FR centroid
                low_mask = (seg == "Low FR").values
                if np.any(low_mask):
                    low_anchor = PCs[low_mask].mean(axis=0)
                    add_biplot_arrows_at_anchor_3d(
                        fig,
                        pca,
                        PCA_FEATURES,
                        low_anchor,
                        scale=1.2,
                        color="green",
                        name_prefix="Low FR ",
                        showlegend=False,
                    )
                # High-FR centroid
                high_mask = (seg == "High FR").values
                if np.any(high_mask):
                    high_anchor = PCs[high_mask].mean(axis=0)
                    add_biplot_arrows_at_anchor_3d(
                        fig,
                        pca,
                        PCA_FEATURES,
                        high_anchor,
                        scale=1.2,
                        color="red",
                        name_prefix="High FR ",
                        showlegend=False,
                    )
            except Exception:
                pass

        st.plotly_chart(fig, use_container_width=True, key=f"{key_prefix}-plot")

        # --- Segment Comparison + FR Correlation ---
        if show_segment_compare:
            c1, c2 = st.columns([2, 1])
            with c1:
                st.markdown("#### Segment Comparison: Low vs High FR")
                st.plotly_chart(
                    segment_diff_bar(df),
                    use_container_width=True,
                    key=f"{key_prefix}-segdiff",
                )
            with c2:
                st.markdown("#### FR Correlation")
                st.plotly_chart(
                    fr_correlation_heatmap(df),
                    use_container_width=True,
                    key=f"{key_prefix}-frcorr",
                )

        # --- Per-selection “Why this run?” ---
        if show_selection_why and highlight_pos is not None:
            st.markdown("#### Why this run?")
            st.plotly_chart(
                per_point_contribution_bar(df, highlight_pos),
                use_container_width=True,
                key=f"{key_prefix}-whybar",
            )

    # === 2) CLUSTER HEATMAP TAB ===
    with tab_cluster:
        st.markdown("#### Cluster-level feature patterns")
        st.plotly_chart(
            cluster_profile_heatmap(df, PCs, scores01),
            use_container_width=True,
            key=f"{key_prefix}-clusterheat",
        )

    # === 3) FEATURE NETWORK TAB ===
    with tab_network:
        # Use the most anomalous subset to focus the network
        n_sub = max(20, int(0.2 * len(df)))
        df_anom = df.sort_values("anomaly_score", ascending=False).head(n_sub)
        st.markdown("#### Relationships among top anomaly-driving features")
        st.plotly_chart(
            feature_influence_network_figure(df_anom),
            use_container_width=True,
            key=f"{key_prefix}-featnet",
        )


