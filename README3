# =========================
# Anomaly methods (helpers)
# =========================
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.neural_network import MLPRegressor

def scale_features(df: pd.DataFrame) -> tuple[np.ndarray, StandardScaler]:
    X = build_features(df).to_numpy(dtype=float)
    scaler = StandardScaler()
    Xz = scaler.fit_transform(X)
    return Xz, scaler

def score_isolation_forest(Xz: np.ndarray, contamination: float = 0.02, n_estimators: int = 300, random_state: int = 0):
    if contamination <= 0 or contamination >= 0.5:
        contamination = "auto"
    model = IsolationForest(
        n_estimators=n_estimators,
        contamination=contamination,
        random_state=random_state,
        n_jobs=-1,
    )
    model.fit(Xz)
    # higher = more normal; use negative to make higher = more anomalous
    raw_scores = -model.score_samples(Xz)
    return model, raw_scores

def score_lof(Xz: np.ndarray, n_neighbors: int = 20):
    # LOF needs n_neighbors < n_samples
    n_neighbors = max(5, min(n_neighbors, max(5, Xz.shape[0] - 1)))
    model = LocalOutlierFactor(
        n_neighbors=n_neighbors,
        novelty=False,  # standard LOF (fit_predict only)
        n_jobs=-1
    )
    # negative_outlier_factor_ (more negative = more anomalous)
    _ = model.fit_predict(Xz)
    raw = -model.negative_outlier_factor_
    return model, raw

def score_autoencoder(Xz: np.ndarray, hidden=(16, 8, 16), alpha=1e-4, max_iter=400, random_state=0):
    """
    Autoencoder via MLPRegressor: train to reconstruct Xz -> Xz.
    Score = reconstruction MSE per row.
    """
    model = MLPRegressor(
        hidden_layer_sizes=hidden,
        activation="relu",
        solver="adam",
        alpha=alpha,
        max_iter=max_iter,
        random_state=random_state,
        shuffle=True,
        early_stopping=True,
        n_iter_no_change=15,
        verbose=False,
    )
    model.fit(Xz, Xz)
    Xz_hat = model.predict(Xz)
    mse = ((Xz - Xz_hat) ** 2).mean(axis=1)
    return model, mse

def normalize_scores(scores: np.ndarray) -> np.ndarray:
    """
    Map arbitrary positive scores to [0,1] robustly (IQR-based).
    """
    s = np.asarray(scores, dtype=float)
    s = np.nan_to_num(s, nan=np.nanmedian(s))
    q1, q3 = np.percentile(s, [25, 75])
    iqr = max(q3 - q1, 1e-9)
    z = (s - q1) / iqr
    z = np.clip(z, 0, None)
    # squashing to [0,1] with 99th percentile cap
    cap = np.percentile(z, 99)
    cap = cap if cap > 0 else 1.0
    return (z / cap).clip(0, 1)

# =========================
# Sidebar (new controls)
# =========================
st.sidebar.header("Controls")

method = st.sidebar.selectbox(
    "Anomaly method",
    ["Isolation Forest", "Local Outlier Factor", "Autoencoder (reconstruction)"],
    index=0
)

# Method-specific params
if method == "Isolation Forest":
    if_contam = st.sidebar.slider("Contamination (expected anomaly %)", 0.0, 0.2, 0.02, 0.005)
    if_trees = st.sidebar.slider("Trees (n_estimators)", 100, 1000, 300, 50)
elif method == "Local Outlier Factor":
    lof_k = st.sidebar.slider("Neighbors (n_neighbors)", 5, 100, 20, 1)
else:  # Autoencoder
    ae_w1 = st.sidebar.slider("Hidden size 1", 4, 64, 16, 2)
    ae_w2 = st.sidebar.slider("Hidden size bottleneck", 2, 32, 8, 1)
    ae_w3 = st.sidebar.slider("Hidden size 3", 4, 64, 16, 2)
    ae_iter = st.sidebar.slider("Max epochs", 100, 1000, 400, 50)

# Existing dataset UI (keep as-is, shown here for continuity)
dataset_names = [p.name for p in csv_paths]
# If you already built dataset_labels with file_name suffix, keep that instead:
# dataset_names = dataset_labels
selected_idx = st.sidebar.selectbox("Dataset", options=list(range(len(dataset_names))),
                                    format_func=lambda i: dataset_names[i], index=0)
show_all = st.sidebar.checkbox("Show all datasets", value=False)
label_choice = st.sidebar.selectbox("Color/Group by", list(LABEL_DICT.keys()))
show_top = st.sidebar.number_input("Show Top-N anomalies table per dataset (0 to hide)",
                                   min_value=0, max_value=1000, value=10, step=1)

# =========================
# render_one: fit/score per method, visualize
# =========================
def render_one(path: Path):
    try:
        raw = load_csv(path)
        df = engineer_features(raw)

        # 1) Scale features once for all methods
        Xz, scaler = scale_features(df)

        # 2) PCA ONLY for visualization
        _, _, _, PCs = fit_pca_scaled(build_features(df).to_numpy(), n_components=3)

        # 3) Score with selected method
        if method == "Isolation Forest":
            _, raw_scores = score_isolation_forest(Xz, contamination=if_contam, n_estimators=if_trees)
        elif method == "Local Outlier Factor":
            _, raw_scores = score_lof(Xz, n_neighbors=lof_k)
        else:
            _, raw_scores = score_autoencoder(Xz, hidden=(ae_w1, ae_w2, ae_w3), max_iter=ae_iter)

        # 4) Normalize scores to [0,1] (higher = more anomalous)
        scores01 = normalize_scores(raw_scores)
        df["anomaly_score"] = scores01
        df["raw_anomaly_score"] = raw_scores

        st.subheader(f"{path.name} Â· {method}")

        # --- Top anomalies table with selection -> highlight on plot ---
        highlight_pos = None
        if show_top > 0:
            cols_keep = ["Test_Run_ID","FR","Num_Tests","total_init_time","anomaly_score","raw_anomaly_score",
                         "Simulation_Host","Simulation_Node","Capability","Script_File","V","s_number","Build"]
            cols_keep = [c for c in cols_keep if c in df.columns]
            top = df.sort_values("anomaly_score", ascending=False).head(show_top)

            st.markdown("**Top anomalies**")
            editable = top[cols_keep].copy()
            editable.insert(0, "Select", False)
            edited = st.data_editor(
                editable,
                use_container_width=True,
                num_rows="fixed",
                disabled={c: True for c in editable.columns if c != "Select"}
            )
            selected_original_idxs = edited.index[edited["Select"]].tolist()
            if selected_original_idxs:
                pos = df.index.get_indexer([selected_original_idxs[0]])
                if len(pos) and pos[0] != -1:
                    highlight_pos = int(pos[0])

        # 5) Plot, pass highlight
        fig = plot_for_df(df, PCs, label_choice, highlight_pos=highlight_pos)
        st.plotly_chart(fig, use_container_width=True)

    except Exception as e:
        st.error(f"Failed to process {path.name}: {e}")
